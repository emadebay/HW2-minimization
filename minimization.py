# -*- coding: utf-8 -*-
"""assignment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158bny8InvhCWHqNFC_q04aQWkpMd01_m
"""

import numpy as np
import mpmath
from mpmath import mp
from decimal import Decimal, getcontext


getcontext().prec = 5000

def function_to_minimize(w1,w2,x,y):

    x = Decimal(x)
    y = Decimal(y)

    w1 = Decimal(w1)
    w2 = Decimal(w2)

    term1 = (x - w1) ** 2
    term2 = (y * (w2 - w1**2) ** 2 )

    term1 = Decimal(term1)
    term2 = Decimal(term2)

    result = term1 + term2

    return Decimal(result)

def derivative_wrt_w1(w1,w2,x,y):
    decimal_places = 2

    x = Decimal(x)
    y = Decimal(y)

    w1 = Decimal(w1)
    w2 = Decimal(w2)

    term1 = -2 * (x - w1)
    term2 = -4 * y * (w1 * (w2 - w1**2))

    term1 = Decimal(term1)
    term2 = Decimal(term2)

    result = term1 + term2

    return result

def derivative_wrt_w2(x, w1, w2, y):
    decimal_places = 2
    x = Decimal(x)
    y = Decimal(y)

    w1 = Decimal(w1)
    w2 = Decimal(w2)

    term1 = 0  # The first term is not dependent on w2.
    term2 = 2 * y * (w2 - w1**2)

    term1 = Decimal(term1)
    term2 = Decimal(term2)

    result = term1 + term2
    return result

def normalize_output(output, min_value, max_value):
    if min_value == max_value:
        # Handle the case where min_value and max_value are the same to avoid division by zero.
        return 0.5  # Midpoint of [0, 1]
    return (output - min_value) / (max_value - min_value)

learning_rate = 0.00001
learning_rate = Decimal(learning_rate)
convergence_threshold = 1e-4

w_1 = 5
w_2 = 5
w1_before = w_1
w2_before = w_2

epoch = 200

x_1 = 5
y_1 = 50

epoch_values = []
loss_history = []

import matplotlib.pyplot as plt

for i in range(0, epoch):

    grad_w1 = derivative_wrt_w1(w_1,w_2,x_1,y_1)
    grad_w2 = derivative_wrt_w2(w_1,w_2,x_1,y_1)

    #updatw w_1, w_2
    w_1 = w_1 - (learning_rate * grad_w1)
    w_2 = w_2 - (learning_rate * grad_w2)

    decimal_places = 2
    w_1 = Decimal(w_1)
    w_2 = Decimal(w_2)


    loss = function_to_minimize(w_1,w_2,x_1,y_1)
    # Append loss and epoch values
    loss = float(loss)
    loss = normalize_output(loss, -10, 10)
    loss_history.append(loss)
    epoch_values.append(i)

    # print(loss)
    # print(i)

plt.plot(epoch_values, loss_history)
plt.xlabel('Epoch')
plt.ylabel('Loss (f(w))')
plt.title('Loss vs. Epoch')
plt.show()

# print(epoch_values, loss_history)
print('x value', x_1)
print('y value', y_1)
print('learning_rate', learning_rate)
print('w1 before and after: ', w1_before, ' &', w_1)
print('w2 before and after: ', w2_before, ' &', w_2)



